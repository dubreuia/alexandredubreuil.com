<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Music Generation with Magenta: Using Machine Learning in Arts</title>
  <link rel="stylesheet"
        href="../common/bower_components/reveal.js/css/reveal.css">
  <link rel="stylesheet"
        href="../common/bower_components/reveal.js/css/theme/white.css">
  <link rel="stylesheet"
        href="../common/css/magenta-theme.css">
  <script
      src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js?skin=desert"></script>
  <style>
    .slide-first footer,
    .slide-last footer {
      background-color: #f5af33;
    }
  </style>
</head>
<body>

<footer id="footer" class="hide">
  <img class="logo"
       src="resources/magenta-logo-01.png"
       alt="Magenta logo">
  <div class="title">
    <div>Music Generation with Magenta</div>
    <div class="subtitle">Using Machine Learning in Arts</div>
  </div>
  <div class="title right">
    <div>Alexandre DuBreuil @dubreuia</div>
    <div class="subtitle">alexandredubreuil.com</div>
  </div>
  <img class="devoxx"
       src="../common/img/devoxx-logo-transparent.png"
       alt="Devoxx logo">
</footer>

<div class="reveal">

  <div class="slides">

    <section>

      <section data-background="../common/img/devoxx-background-03.png"
               data-background-size="contain"
               data-background-position="left">
        <img width="250px"
             class="logo figure no_border"
             src="resources/magenta-logo-02.png"
             alt="Magenta logo">
        <h1 class="align-left">
          Music Generation with <strong style="color:#4c1130ff">Magenta</strong>
        </h1>
        <h2 class="align-right">Using Machine Learning in Arts</h2>
        <h3 class="align-right">Alexandre DuBreuil</h3>
        <h4 class="align-right">@dubreuia</h4>
      </section>

      <section>
        <h3>Alexandre DuBreuil</h3>
        <p>
          Software engineer, sound designer, conference speaker and open source
          maintainer.
        </p>
        <p>@dubreuia</p>
      </section>

    </section>

    <section>

      <section>
        <h3>Generative Music</h3>
        <p>(in 10 minutes)</p>
      </section>

      <section>
        <p>
          "Generative art is an artwork partially or completely
          created by an autonomous system."
        </p>
      </section>

      <section data-background="resources/ableton-live-02.jpg"
               class="background">
        <h4>
          Make music without being a musician
        </h4>
        <p>
          Maybe you don't know how to <strong>improvise</strong>, maybe you
          need help to <strong>compose</strong>.
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg">https://media.pitbullaudio.com/media/catalog/product/cache/1/image/9df78eab33525d08d6e5fb8d27136e95/s/u/suite_3_1.jpeg</a></span>
        </p>
      </section>

      <section data-background="resources/silly-01.jpg"
               class="background">
        <h4>
          <a href="https://www.youtube.com/watch?v=DkiFjzQgJtg">Monica
            Dinculescu - Why you should build silly things</a>
        </h4>
        <p>
          It is okay to make art without taking ourselves seriously. Generative
          music is a good way of doing that, because those kind of systems can
          be <strong>interacted with</strong> easily.
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://www.corelogic.com.au/sites/default/files/2018-08/1200px-800px--tworobots.jpg">https://www.corelogic.com.au/sites/default/files/2018-08/1200px-800px--tworobots.jpg</a></span>
        </p>
      </section>

      <section data-background="resources/system-01.jpg"
               class="background">
        <h4>
          Helping people build generative systems
        </h4>
        <p>
          To make generative art, you need <strong>autonomous systems</strong>
          that makes it possible. As software developers, this is our role
          to provide that, whether it is for an art exhibit, a generative
          radio, or a fun website.
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://i.pinimg.com/originals/07/3e/86/073e862a13e4bacc7589f0d4eff4b873.jpg">https://i.pinimg.com/originals/07/3e/86/073e862a13e4bacc7589f0d4eff4b873.jpg</a></span>
        </p>
      </section>

      <section data-background="resources/brian-eno.jpg"
               class="background">
        <h4>
          "The weird and the strange is good" - Brian Eno
        </h4>
        <p>
          Generative systems makes tons of mistakes (also humans), <strong>but
          mistakes are good</strong>.
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://i.ytimg.com/vi/Dwo-tvmEKhk/maxresdefault.jpg">https://i.ytimg.com/vi/Dwo-tvmEKhk/maxresdefault.jpg</a></span>
        </p>
      </section>

      <section>
        <h4>Machine Learning</h4>
        <p>
          Hand crafting the rules of a painting or the rules of a music style
          might be a hard task. That's why Machine Learning is so interesting
          in arts: it can learn complex functions.
        </p>
      </section>

      <section>
        <h4>Representation: MIDI</h4>
        <p>
          MIDI is a musical representation analogous to <strong>sheet
          music</strong>, where note has a pitch, velocity, and time.
        </p>
        <p>
          Working with MIDI shows the underlying <strong>structure of the
          music</strong>, but doesn't define the actual sound, you'll need to
          use instruments (numeric or analogic).
        </p>
        <img width="90%"
             src="resources/midi.png"
             alt="MIDI diagram"/>
      </section>

      <section>
        <h4>Representation: audio</h4>
        <p>
          Working with audio is harder because you have to handle 16000
          samples per seconds (at least) and keep track of the general
          structure. Generating audio is more direct than MIDI.
        </p>
        <p>
          <img width="50%"
               src="resources/spectrogram.png"
               alt="Audio diagram"/>
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png">https://upload.wikimedia.org/wikipedia/commons/c/c5/Spectrogram-19thC.png</a></span>
        </p>
      </section>

      <section class="align-left">
        <h4>Music generation with RNNs (MIDI)</h4>
        <p>
          <img width="250"
               class="figure"
               src="resources/rnn.png"
               alt="RNN diagram"/>
          Recurrent Neural Networks (RNNs) solves two important properties for
          music generation: they <strong>operate on sequences for the inputs
          and outputs</strong> and they <strong>can remember past
          events</strong>.
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://www.asimovinstitute.org/wp-content/uploads/2016/09/rnn.png">www.asimovinstitute.org/wp-content/uploads/2016/09/rnn.png</a></span>
        </p>
      </section>

      <section class="align-left">
        <h4>Long-term structure with LSTMs (MIDI)</h4>
        <p>
          <img width="250px"
               class="figure"
               src="resources/lstm.png"
               alt="LSTM diagram"/>
          Most RNN uses Long Short-Term Memory (LSTM) cells, since by
          themselves, RNNs are hard to train because of the problems of
          vanishing and exploding gradient, making long-term dependencies
          hard to learn.
        </p>
        <p>
          By using <strong>input, output and forget gates</strong> in
          the cell, LSTMs can learn mechanisms to keep or forget information
          as they go.
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://www.asimovinstitute.org/wp-content/uploads/2016/09/lstm.png">https://www.asimovinstitute.org/wp-content/uploads/2016/09/lstm.png</a></span>
        </p>
      </section>

      <section class="align-left">
        <h4>Latent space interpolation with VAEs (MIDI)</h4>
        <p>
          <img width="250px"
               class="figure"
               src="resources/vae.png"
               alt="VAE diagram"/>
          Variational Autoencoders (VAEs) are a pair of networks where an
          encoder reduces the input to a lower dimensionality (<strong>latent
          space</strong>), from which a decoder tries to reproduce the
          input.
        </p>
        <p>
          The latent space is continuous and follows a probability
          distribution, meaning it is possible to sample from it.
          VAEs are inherently generative models: they can
          <strong>sample</strong> and <strong>interpolate</strong>
          (smoothly move in the latent space) between two points.
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://www.asimovinstitute.org/wp-content/uploads/2016/09/vae.png">https://www.asimovinstitute.org/wp-content/uploads/2016/09/vae.png</a></span>
        </p>
      </section>

      <section class="align-left">
        <h4>Audio generation with WaveNet Autoencoders (audio)</h4>
        <p>
          <img width="500px"
               class="figure"
               src="resources/wavenet.png"
               alt="Wavenet diagram"/>
          WaveNet is a convolutional neural network (CNN) taking raw signal
          as an input and synthesizing output audio sample by sample.
        </p>
        <p>
          The WaveNet Autoencoder present in Magenta is a Wavenet-style AE
          network capable of learning its own temporal embedding, resulting
          in a latent space from which is it possible to <strong>sample
        </strong> and <strong>mix</strong> elements.
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://magenta.tensorflow.org/assets/nsynth_05_18_17/encoder-decoder.png">magenta.tensorflow.org/assets/nsynth_05_18_17/encoder-decoder.png</a></span>
        </p>
      </section>

      <section>
        <h4>What's in Magenta?</h4>
        <table class="smaller">
          <thead>
          <tr>
            <th>Model</th>
            <th>Network</th>
            <th>Repr.</th>
            <th>Encoding</th>
          </tr>
          </thead>
          <tbody>
          <tr>
            <td>DrumsRNN</td>
            <td>LSTM</td>
            <td>MIDI</td>
            <td>polyphonic-ish</td>
          </tr>
          <tr>
            <td>MelodyRNN</td>
            <td>LSTM</td>
            <td>MIDI</td>
            <td>monophonic</td>
          </tr>
          <tr>
            <td>PolyphonyRNN</td>
            <td>LSTM</td>
            <td>MIDI</td>
            <td>polyphonic</td>
          </tr>
          <tr>
            <td>PerformanceRNN</td>
            <td>LSTM</td>
            <td>MIDI</td>
            <td>polyphonic, groove</td>
          </tr>
          <tr>
            <td>MusicVAE</td>
            <td>VAE</td>
            <td>MIDI</td>
            <td>multiple</td>
          </tr>
          <tr>
            <td>NSynth</td>
            <td>Wavenet AE</td>
            <td>Audio</td>
            <td>-</td>
          </tr>
          <tr>
            <td>GANSynth</td>
            <td>GAN</td>
            <td>Audio</td>
            <td>-</td>
          </tr>
          </tbody>
        </table>
      </section>

    </section>

    <section>

      <section>
        <h3>Live code: Generate a track</h3>
        <p>(in 20 minutes)</p>
      </section>

      <section data-background="resources/cat-on-a-keyboard-in-space.jpg"
               class="background">
        <h4>
          STEP 1: make everything sound like a cat.
        </h4>
        <p>
          We'll use NSynth to mix cat sounds with other sounds.
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://images6.alphacoders.com/475/475224.jpg">https://images6.alphacoders.com/475/475224.jpg</a></span>
        </p>
      </section>

      <section>
        <h4>STEP 1: The sounds</h4>
        <div class="sound_clip">
          <img src="resources/rainbowgram-bass-01.png"
               class="no_border"
               alt="Audio diagram bass"/>
          <audio controls>
            <source src="code/sounds/83249__zgump__bass-0205__crop.wav"
                    type="audio/wav"/>
          </audio>
        </div>
        <div class="sound_clip">
          <img src="resources/rainbowgram-metal-01.png"
               class="no_border"
               alt="Audio diagram metal"/>
          <audio controls>
            <source
                src="code/sounds/160045__jorickhoofd__metal-hit-with-metal-bar-resonance__crop.wav"
                type="audio/wav"/>
          </audio>
        </div>
        <div class="sound_clip">
          <img src="resources/rainbowgram-cat-01.png"
               class="no_border"
               alt="Audio diagram cat"/>
          <audio controls>
            <source src="code/sounds/412017__skymary__cat-meow-short__crop.wav"
                    type="audio/wav"/>
          </audio>
        </div>
        <div class="sound_clip">
          <img src="resources/rainbowgram-flute-01.png"
               class="no_border"
               alt="Audio diagram flute"/>
          <audio controls>
            <source src="code/sounds/427567__maria-mannone__flute__crop.wav"
                    type="audio/wav"/>
          </audio>
        </div>
      </section>

      <section>
        <h4>STEP 1: Entry point</h4>
        <p>
          See "code/nsynth.py" and method <code>app</code> in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def app(unused_argv):
  encoding1, encoding2 = encode([FLAGS.wav1, FLAGS.wav2],
                                sample_length=FLAGS.sample_length,
                                sample_rate=FLAGS.sample_rate,
                                checkpoint=FLAGS.checkpoint)
  encoding_mix = mix(encoding1, encoding2)
  synthesize(encoding_mix, checkpoint=FLAGS.checkpoint)
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>STEP 1: Encode</h4>
        <p>
          See "code/nsynth.py" and method <code>encode</code> in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def encode(paths: List[str],
           sample_length: int = 16000,
           sample_rate: int = 16000,
           checkpoint: str = "checkpoints/wavenet-ckpt/model.ckpt-200000") \
    -> np.ndarray:
  audios = []
  for path in paths:
    audio = utils.load_audio(path,
                             sample_length=sample_length,
                             sr=sample_rate)
    audios.append(audio)
  audios = np.array(audios)
  encodings = fastgen.encode(audios, checkpoint, sample_length)
  return encodings
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>STEP 1: Mix</h4>
        <p>
          See "code/nsynth.py" and method <code>mix</code> in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def mix(encoding1: np.ndarray,
        encoding2: np.ndarray) \
    -> np.ndarray:
  encoding_mix = (encoding1 + encoding2) / 2.0
  return encoding_mix
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>STEP 1: Synthesize</h4>
        <p>
          See "code/nsynth.py" and method <code>synthesize</code> in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def synthesize(encoding_mix: np.ndarray,
               checkpoint: str = "checkpoints/wavenet-ckpt/model.ckpt-200000"):
  os.makedirs(os.path.join("output", "synth"), exist_ok=True)
  date_and_time = time.strftime("%Y-%m-%d_%H%M%S")
  output = os.path.join("output", "synth", f"{date_and_time}.wav")
  encoding_mix = np.array([encoding_mix])
  fastgen.synthesize(encoding_mix,
                     checkpoint_path=checkpoint,
                     save_paths=[output])
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>STEP 1: GANSynth</h4>
        <p>
          As shown, the NSynth instrument is nice, but really slow for the
          audio synthesis. You should use <strong>GANSynth</strong>.
        </p>
      </section>

      <section>
        <h4>STEP 1: The results</h4>
        <div>
          <div class="sound_clip smaller">
            <img src="resources/83249_412017_rainbowgram_trim.png"
                 class="no_border"
                 alt="Audio diagram bass cat"/>
            <span>Bass + Cat</span>
            <audio controls>
              <source src="code/sounds/83249_412017_bass_cat.wav"
                      type="audio/wav"/>
            </audio>
          </div>
          <div class="sound_clip smaller">
            <img src="resources/83249_427567_rainbowgram_trim.png"
                 class="no_border"
                 alt="Audio diagram bass flute"/>
            <span>Bass + Flute</span>
            <audio controls>
              <source src="code/sounds/83249_427567_bass_flute.wav"
                      type="audio/wav"/>
            </audio>
          </div>
          <div class="sound_clip smaller">
            <img src="resources/83249_160045_rainbowgram_trim.png"
                 class="no_border"
                 alt="Audio diagram bass metal"/>
            <span>Bass + Metal</span>
            <audio controls>
              <source src="code/sounds/83249_160045_bass_metal.wav"
                      type="audio/wav"/>
            </audio>
          </div>
        </div>
        <div>
          <div class="sound_clip smaller">
            <img src="resources/160045_83249_rainbowgram_trim.png"
                 class="no_border"
                 alt="Audio diagram metal bass"/>
            <span>Metal + Bass</span>
            <audio controls>
              <source src="code/sounds/160045_83249_metal_bass.wav"
                      type="audio/wav"/>
            </audio>
          </div>
          <div class="sound_clip smaller">
            <img src="resources/160045_412017_rainbowgram_trim.png"
                 class="no_border"
                 alt="Audio diagram metal cat"/>
            <span>Metal + Cat</span>
            <audio controls>
              <source src="code/sounds/160045_412017_metal_cat.wav"
                      type="audio/wav"/>
            </audio>
          </div>
          <div class="sound_clip smaller">
            <img src="resources/160045_427567_rainbowgram_trim.png"
                 class="no_border"
                 alt="Audio diagram metal flute"/>
            <span>Metal + Flute</span>
            <audio controls>
              <source src="code/sounds/160045_427567_metal_flute.wav"
                      type="audio/wav"/>
            </audio>
          </div>
        </div>
      </section>

      <section data-background="resources/cats.jpg"
               class="background">
        <h4>
          STEP 2: sequence the cats
        </h4>
        <p>
          We'll use DrumsRNN and MelodyRNN to generate MIDI to play the
          samples.
        </p>
        <p>
          <span class="figure-caption"><a
              href="http://wallpaperstock.net/cats-line-up_wallpapers_13044_1920x1200.jpg">http://wallpaperstock.net/cats-line-up_wallpapers_13044_1920x1200.jpg</a></span>
        </p>
      </section>

      <section>
        <h4>STEP 2: Reset</h4>
        <p>
          See "code/sequences.py" and method <code>reset</code> in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def reset(loop_start_time: float,
          loop_end_time: float,
          seconds_per_loop: float):
  sequence = music_pb2.NoteSequence()
  sequence = loop(sequence,
                  loop_start_time,
                  loop_end_time,
                  seconds_per_loop)
  return sequence
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>STEP 2: Loop</h4>
        <p>
          See "code/sequences.py" and method <code>loop</code> in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def loop(sequence: NoteSequence,
         loop_start_time: float,
         loop_end_time: float,
         seconds_per_loop: float):
  sequence = ss.trim_note_sequence(sequence,
                                   loop_start_time,
                                   loop_end_time)
  sequence = ss.shift_sequence_times(sequence,
                                     seconds_per_loop)
  return sequence
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>STEP 2: Generate</h4>
        <p>
          See "code/sequences.py" and method <code>generate</code> in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def generate(sequence: NoteSequence,
             name: str,
             bundle_filename: str,
             config_name: str,
             generation_start_time: float,
             generation_end_time: float):
  generator_options = generator_pb2.GeneratorOptions()
  generator_options.args['temperature'].float_value = 1
  generator_options.generate_sections.add(
    start_time=generation_start_time,
    end_time=generation_end_time)
  sequence_generator = get_sequence_generator(name,
                                              bundle_filename,
                                              config_name)
  sequence = sequence_generator.generate(sequence,
                                         generator_options)
  sequence = ss.trim_note_sequence(sequence,
                                   generation_start_time,
                                   generation_end_time)
  return sequence
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>STEP 2: Sequence generator</h4>
        <p>
          See "code/sequences.py" and method
          <code>get_sequence_generator</code> in this repo.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-python" data-trim data-noescape>
def get_sequence_generator(name: str,
                           bundle_filename: str,
                           config_name: str):
  if name == "drums":
    generator = drums_rnn_sequence_generator
  elif name == "melody":
    generator = melody_rnn_sequence_generator
  else:
    raise Exception(f"Unknown sequence generator {name}")

  mm.notebook_utils.download_bundle(bundle_filename, "bundles")
  bundle = mm.sequence_generator_bundle.read_bundle_file(
    os.path.join("bundles", bundle_filename))

  generator_map = generator.get_generator_map()
  sequence_generator = generator_map[config_name](
    checkpoint=None, bundle=bundle)
  sequence_generator.initialize()

  return sequence_generator
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Wrapping up</h4>
        <p>
          This generative music demo helped us improvise and compose around an
          idea: a track composed of a percussion, a melody, and a cat
          &#x1F63A;. We could <strong>interact</strong> with the system
          and it helped us improvise around a theme.
        </p>
      </section>

      <section>
        <h4>Can we do better?</h4>
        <p>
          Was it perfect? No, we had little happy accidents.
        </p>
        <div class="fragment">
          <img width="25%"
               src="resources/bob-ross.jpg"
               alt="Bob Ross image"/>
          <div>
          <span class="figure-caption"><a
              href="https://2.bp.blogspot.com/_s5_5vgBh1Zo/TJVmnBJW-bI/AAAAAAAAAHc/Mcgu8_el_84/s1600/Bob_Ross.jpg">https://2.bp.blogspot.com/_s5_5vgBh1Zo/TJVmnBJW-bI/AAAAAAAAAHc/Mcgu8_el_84/s1600/Bob_Ross.jpg</a></span>
          </div>
        </div>
        <p>
          Maybe we could have had more control over the sequences. Maybe we
          wanted to improvise around a <strong>musical style</strong>,
          or a <strong>specific structure</strong>.
        </p>
      </section>

    </section>

    <section>

      <section>
        <h3>Training</h3>
        <p>(in 5 minutes)</p>
      </section>

      <section>
        <h4>Why?</h4>
        <p>
          The pre-trained models in Magenta are good, but if for example
          you want to generate a <strong>specific style</strong>, generate a
          <strong>specific time signature</strong> (3/4 for example), or a
          <strong>specific instrument</strong> (cello for example) you'll
          need to train your own.
        </p>
      </section>

      <section>
        <h4>Datasets: LAKHS (MIDI)</h4>
        <p>
          A good place to start is the LAKHS dataset, a 180,000 MIDI files
          dataset, (partially) matched with the Million Song Dataset
          (metadata like artist, release, genre).
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://colinraffel.com/projects/lmd/">https://colinraffel.com/projects/lmd/</a></span>
        </p>
        <p>
          <span class="figure-caption"><a
              href="http://millionsongdataset.com/">http://millionsongdataset.com/</a></span>
        </p>
      </section>

      <section>
        <h4>Datasets: NSynth (audio)</h4>
        <p>
          A large-scale and high-quality dataset of annotated musical notes.
          Training audio requires lots of resources, but can be achieved using
          GANSynth.
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://magenta.tensorflow.org/datasets/nsynth">https://magenta.tensorflow.org/datasets/nsynth</a></span>
        </p>
      </section>

      <section>
        <h4>Building the dataset</h4>
        <p>
          From MIDI, ABCNotation, MusicXML files to <code>NoteSequence</code>.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-bash smaller" data-trim data-noescape>
convert_dir_to_note_sequences \
  --input_dir="/path/to/dataset/jazz_midi/drums/v1" \
  --output_file="/tmp/notesequences.tfrecord" \
  --recursive
            </code>
          </pre>
        </div>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-bash smaller" data-trim data-noescape>
...
Converted MIDI file /path/to/dataset/jazz_midi/drums/v1/TRVUCSW12903CF536D.mid.
Converted MIDI file /path/to/dataset/jazz_midi/drums/v1/TRWSJLM128F92DF651.mid.
...
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Create SequenceExamples</h4>
        <p>
          The sequence examples are fed into the model, it contains a sequence
          of inputs and a sequence of labels that represents the drum track.
          Those are split to eval and training sets.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-bash smaller" data-trim data-noescape>
drums_rnn_create_dataset \
  --config="drum_kit" \
  --input="/tmp/notesequences.tfrecord" \
  --output_dir="/tmp/drums_rnn/sequence_examples" \
  --eval_ratio=0.10
            </code>
          </pre>
        </div>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-bash smaller" data-trim data-noescape>
...
DAGPipeline_DrumsExtractor_training_drum_track_lengths_in_bars:
  [8,10): 1
  [10,20): 8
  [20,30): 8
  [30,40): 29
  [40,50): 1
  [50,100): 2
...
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Train and evaluate the model</h4>
        <p>
          Launch the training, using a specific configuration and
          hyperparameters.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-bash smaller" data-trim data-noescape>
drums_rnn_train \
  --config="drum_kit" \
  --run_dir="/tmp/drums_rnn/logdir/run1" \
  --sequence_example_file="/tmp/drums_rnn/sequence_examples/training_drum_tracks.tfrecord" \
  --hparams="batch_size=64,rnn_layer_sizes=[64,64]" \
  --num_training_steps=20000
            </code>
          </pre>
        </div>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-bash smaller" data-trim data-noescape>
...
Saving checkpoints for 0 into /tmp/drums_rnn/logdir/run1/train/model.ckpt.
Accuracy = 0.013341025, Global Step = 1, Loss = 6.2323294, Perplexity = 508.9396
Accuracy = 0.43837976, Global Step = 11, Loss = 5.1239195, Perplexity = 167.99252 (50.009 sec)
global_step/sec: 0.199963
Saving checkpoints for 12 into /tmp/drums_rnn/logdir/run1/train/model.ckpt.
...
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Tensorboard</h4>
        <p>
          You can launch TensorBoard and go to http://localhost:6006 to view
          the TensorBoard dashboard.
        </p>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-bash smaller" data-trim data-noescape>
tensorboard --logdir="/tmp/drums_rnn/logdir"
            </code>
          </pre>
        </div>
      </section>

    </section>

    <section>

      <section>
        <h3>Interaction with the outside world</h3>
        <p>(in 5 minutes)</p>
      </section>

      <section>
        <h4>Python to everything using MIDI</h4>
        <p>
          Magenta can send MIDI, which is understood by basically everything
          that makes sound: <strong>DAWs</strong> (like Ableton Live),
          <strong>software synthesizers</strong> (like fluidsynth),
          <strong>hardware synthesizers</strong> (though USB or MIDI cable),
          etc.
        </p>
      </section>

      <section>
        <h4>Magenta in the browser with Magenta.js</h4>
        <p>
          You can use Magenta and most of its models in the browser, using
          Magenta.js (which in turns uses Tensorflow.js).
        </p>
        <p>
          Amazing project to share generative music applications. Harder
          to use the audio and MIDI in other softwares like DAWs.
        </p>
      </section>

      <section>
        <h4>Melody Mixer</h4>
        <p>
          <img
              width="75%"
              src="resources/melody-mixer.png"
              alt="Melody mixer"/>
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://experiments.withgoogle.com/ai/melody-mixer/view/">https://experiments.withgoogle.com/ai/melody-mixer/view/</a></span>
        </p>
      </section>

      <section>
        <h4>Neural Drum Machine</h4>
        <p>
          <img
              width="75%"
              src="resources/neural-drum-machine.png"
              alt="Neural Drum Machine"/>
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://codepen.io/teropa/full/JLjXGK">https://codepen.io/teropa/full/JLjXGK</a></span>
        </p>
      </section>

      <section>
        <h4>GANHarp</h4>
        <p>
          <img
              width="75%"
              src="resources/ganharp.png"
              alt="GANHarp"/>
        </p>
        <p>
          <span class="figure-caption"><a
              href="https://ganharp.ctpt.co">https://ganharp.ctpt.co</a></span>
        </p>
      </section>

      <section>
        <h4>Easy peasy</h4>
        <div class="code-wrapper">
          <pre class="prettyprint">
            <code class="code lang-html" data-trim data-noescape>
&lt;html&gt;
  &lt;head&gt;
    &lt;!-- Load @magenta/music --&gt;
    &lt;script src="https://cdn.jsdelivr.net/npm/@magenta/<mark
                class="fragment highlight-current-blue">music@^1.0.0</mark>"&gt
    &lt;/script&gt;
    &lt;script&gt;
      // Instantiate model by loading desired config.
      const model = new mm.<mark class="fragment highlight-current-blue">MusicVAE</mark>(
        'https://storage.googleapis.com/magentadata/' +
        'js/checkpoints/music_vae/<mark class="fragment highlight-current-blue">trio_4bar</mark>');
      const player = new mm.Player();

      // Samples from MusicVAE and play the sample
      function play() {
        player.resumeContext();
        model.<mark class="fragment highlight-current-blue">sample(1)</mark>
          .then((samples) =&gt; player.start(samples[0], 80));
      }
    &lt;/script&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;button onclick="play()"&gt;&lt;h1&gt;Play Trio&lt;/h1&gt;&lt;/button&gt;&lt;/body&gt;
&lt;/html&gt;
            </code>
          </pre>
        </div>
      </section>

      <section>
        <h4>Magenta in your DAW with Magenta Studio</h4>
        <p>
          Using Magenta.js and Max4Live (MaxMSP) process, Magenta Studio can
          be used directly in Ableton Live.
        </p>
        <p>
          <video autoplay="" loop="" muted="" playsinline="">
            <source src="../common/video/hero.mp4" type="video/mp4">
          </video>
          <span class="figure-caption"><a
              href="https://magenta.tensorflow.org/studio/assets/studio/hero.mp4">https://magenta.tensorflow.org/studio/assets/studio/hero.mp4</a></span>
        </p>
      </section>

    </section>

    <section>

      <section>
        <h3>Closing</h3>
      </section>

      <section>
        <h4>Dreambank - Can a machine dream?</h4>
        <p>
          Generative music using Magenta and Ableton Live, exhibit by
          Claire Malrieux.
        </p>
        <img width="33%"
             src="resources/dreambank.jpg"
             alt="Dreambank image"/>
      </section>

      <section>
        <h4>Hands-on Music Generation with Magenta</h4>
        <p>
          Upcoming book on <strong>Packt Publishing</strong>, expected
          publication date in <strong>January 2020</strong>. Easy to read for
          non-musicians.
        </p>
        <img width="25%"
             class="no_border"
             src="resources/magenta-book-icon.png"
             alt="Magenta book icon"/>
        <img width="25%"
             class="no_border"
             src="resources/packt-logo.png"
             alt="Packt logo"/>
      </section>

      <section>
        <h4>
          <a href="https://alexandredubreuil.com">https://alexandredubreuil.com</a>
        </h4>
        <p>
          <strong>Slides</strong> and <strong>source code</strong> of
          this presentation, blog articles on Magenta, book references, etc.
        </p>
      </section>

    </section>

    <!--
    TODO
    - add generate for the training at the end
    -->

    <section>

      <section data-background="../common/img/devoxx-background-03.png"
               data-background-size="contain"
               data-background-position="left">
        <h3>Thank you!</h3>
      </section>

    </section>

  </div>
</div>
<script src="../common/bower_components/reveal.js/js/reveal.js"></script>
<script src="../common/bower_components/jquery/dist/jquery.js"></script>
<script src="../common/js/reveal.js"></script>
<script>
    Reveal.initialize({
        progress: true,
        history: true,
    });
</script>
</body>
</html>
